<div align=center>
    <img src="assets/logo1.png" width=15%>
    <h1>MODA: MOdular Duplex Attention for <br>Multimodal Perception, Cognition, and Emotion Understanding</h1>

<div class="is-size-5 publication-authors">
<span class="author-block">
    <a href="https://zzcheng.top/" target="_blank">Zhicheng Zhang</a><sup>1,2,â€ </sup>,
</span>
<span class="author-block">
    Wuyou Xia<sup>1</sup>,
</span>
<span class="author-block">
    Chenxi Zhao<sup>1,â€ </sup>,
</span>
<span class="author-block">
    Yan Zhou<sup>3</sup>,
</span>
<span class="author-block">
    Xiaoqiang Liu<sup>3</sup>,
</span>
<span class="author-block">
    <a href="https://yongjie-zhu.github.io/" target="_blank">Yongjie Zhu</a><sup>3,â€¡</sup>,
</span>
<span class="author-block">
    Wenyu Qin<sup>3</sup>,
</span>
<span class="author-block">
    <a href="https://scholar.google.com/citations?user=P6MraaYAAAAJ&hl=en/" target="_blank">Pengfei Wan</a><sup>3</sup>,
</span>
<span class="author-block">
    Di Zhang<sup>3</sup>,
</span>
<span class="author-block">
    <a href="https://cv.nankai.edu.cn/" target="_blank">Jufeng Yang</a><sup>1,2,âœ‰</sup>
</span>
</div>


<div class="is-size-5 publication-authors">
<sup>1</sup><span class="author-block">Nankai University</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<sup>2</sup><span class="author-block">Pengcheng Laboratory</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<sup>3</sup><span class="author-block">Kuaishou Technology</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
</div>

<sup>â€ </sup><span class="author-block">Work done at KlingAI</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
            <sup>â€¡</sup><span class="author-block">Project Leader</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
            <sup>âœ‰</sup><span class="author-block">Corresponding Author</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;


**ğŸ‰ Accepted by [ICML 2025 Spotlight](hhttps://icml.cc/virtual/2025/poster/46210) ğŸ‰**

[ğŸ“ƒ [Paper](https://arxiv.org/abs/2507.04635) ]
[ğŸ“¦ [Code](https://github.com/KwaiVGI/MODA) ]
[âš’ï¸ [Project](https://zzcheng.top/MODA) ]
[ğŸ“… [Slide](https://zzcheng.top/assets/pdf/2025_ICML_MODA_slide.pdf) ]
<!-- [ğŸ“Š [Poster](https://zzcheng.top/assets/pdf/2024_CVPR_ExtDM_poster.pdf) ] -->
<!-- [ğŸ“ƒ [ä¸­è¯‘ç‰ˆ](https://zzcheng.top/assets/pdf/2024_CVPR_ExtDM_chinese.pdf) ] -->
<!-- [ğŸï¸ [Video](https://www.bilibili.com/video/BV1dC411E72q) / [YouTube](https://www.youtube.com/watch?v=1hxOUagr8mM) ] -->

<img src="assets/pipeline.png" width=400 />
</div>

> **TL;DR**: We i) identify attention deficit disorder as a critical barrier hindering fine-grained content understanding in MLLMs; ii) introduce a modular duplex attention mechanism to mitigate modality bias and enhance attention score justification; and iii) develop MODA-based MLLMs that enable fine-grained multimodal understanding across perception, cognition, and emotion tasks.

## ğŸ“ˆ 1. News

- ğŸ”¥2024-07-10: Creating repository. The code is uploading ...
- 2024-05-01: MODA has been accepted to ICML 2025ï¼